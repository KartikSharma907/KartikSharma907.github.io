<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Kartik Sharma</title>

    <meta name="author" content="Kartik Sharma">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/site-logo.png" type="image/png">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Kartik Sharma
                </p>
                <p style="text-align: center; font-size: 24px; font-weight: 700; margin: 20px 0;">
                  Master's in Robotics @ CMU
                </p>
                <p>
                  I'm Kartik Sharma, a Master's in Robotics student at Carnegie Mellon University (Fall 2025), passionate about building intelligent systems that learn from — and reason across — diverse forms of data. My academic foundation in Computer Science and Economics from BITS Pilani has given me both the technical skills and analytical perspective to tackle complex AI challenges.
                </p>
                <p>
                  My research spans computer vision, natural language processing, and multi-modal deep learning, with a strong focus on making AI systems more ethical, transparent, and deployable in real-world settings. I've published at IEEE Big Data and CVPR, worked on generative models and safety-critical LLM filters, and deployed language-vision models on millions of Samsung devices worldwide.
                </p>
                <p>
                  Right now, I'm expanding my expertise in multi-modal reasoning and its applications in robotics — aiming to design systems that simplify human tasks through intelligent perception and decision-making. My goal is to bridge cutting-edge research with impactful applications, ensuring AI benefits people at scale.
                </p>
                <p style="text-align:center">
                  <a href="mailto:f20180789@pilani.bits-pilani.ac.in">Email</a> &nbsp;/&nbsp;
                  <a href="data/CV_Kartik Sharma.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="data/Resume Kartik Sharma.pdf">Resume</a> &nbsp;/&nbsp;
                  <a href="projects.html">Projects</a> &nbsp;/&nbsp;
                  <a href="publications.html">Publications</a> &nbsp;/&nbsp;
                  <a href="blog/">Blog</a> &nbsp;/&nbsp;
                  <a href="contact.html">Contact</a> &nbsp;/&nbsp;
                  <a href="https://github.com/KartikSharma907">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/kartik-sharma-907/">LinkedIn</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/bio-photo-2.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/bio-photo-2.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          
          <!-- Research Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <h3>The Big Picture</h3>
                <p>
                  I am fascinated by how humans integrate different sensory signals — sight, sound, language — to understand and interact with the world. My research seeks to replicate this capability in artificial systems, enabling them to process and reason across multiple modalities of data. This includes advancing methods in multi-modal deep learning for tasks like language-vision reasoning, domain adaptation in real-world environments, and robust performance in long-tailed data distributions. I also aim to embed ethical frameworks into AI, ensuring fairness, transparency, and alignment with societal values.
                </p>
                <h3>My Approach</h3>
                <p>
                  I combine computer vision, natural language processing, and generative modeling techniques to tackle these challenges. My work has explored diverse methodologies — from contrastive learning for cross-lingual object grounding, to GAN-based strategies for generating diverse images from limited data, to retrieval-augmented generation pipelines for large language models. I focus on designing architectures and training strategies that improve generalization, reduce biases, and scale efficiently to deployment. This includes bridging discriminative and generative paradigms, leveraging data augmentation, and ensuring robustness to adversarial inputs.
                </p>
                <h3>Current & Future Work</h3>
                <p>
                  Currently, I'm expanding my research at CMU into multi-modal reasoning for robotics — integrating perception, language understanding, and decision-making to create systems that can adapt to dynamic environments. I am particularly interested in aligning diffusion and GAN-based generation models with reward signals for more controllable outputs, and in developing multi-modal architectures capable of reliable reasoning across diverse contexts. Looking ahead, I plan to explore how these systems can be deployed in the physical world, with a focus on assisting humans in high-stakes, complex tasks where safety, adaptability, and transparency are paramount.
                </p>
              </td>
            </tr>
          </tbody></table>

          <!-- Publications Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
                <p>
                  <strong>NoisyTwins: Class-Consistent and Diverse Image Generation through StyleGANs</strong><br>
                  Harsh Rangwani, Lavish Bansal, Kartik Sharma, Tejan Karmali, Varun Jampani, R. Venkatesh Babu<br>
                  <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2023)</em><br>
                  <a href="https://arxiv.org/pdf/2304.05866">Link to Paper</a> / 
                  <a href="https://rangwani-harsh.github.io/NoisyTwins/">Project Page</a> / 
                  <a href="https://github.com/val-iisc/NoisyTwins">Code</a>
                </p>
                <p>
                  <strong>A Generalized Multimodal Deep Learning Model for Early Crop Yield Prediction</strong><br>
                  Arshveer Kaur, Poonam Goyal, Kartik Sharma, Lakshay Sharma, Navneet Goyal<br>
                  <em>IEEE International Conference on Big Data (Big Data 2022)</em><br>
                  <a href="https://www.researchgate.net/profile/Kartik-Sharma-35/publication/367456295_A_Generalized_Multimodal_Deep_Learning_Model_for_Early_Crop_Yield_Prediction/links/668311410a25e27fbc1a78a9/A-Generalized-Multimodal-Deep-Learning-Model-for-Early-Crop-Yield-Prediction.pdf">Link to Paper</a>
                </p>
              </td>
            </tr>
          </tbody></table>

          <!-- Experience Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Experience</h2>
                <p>
                  <strong>Research Engineer</strong> — Samsung R&D Institute India (Nov 2023 – Present)<br>
                  In the Language AI team, I developed and optimized a multilingual safety filter for Samsung's LLM, achieving 95% accuracy across 12+ locales and protecting against prompt injection, gibberish attacks, and data leaks. I led the quantization and on-device deployment of models with a 45% size reduction, enabling real-time performance on flagship devices. I also implemented cross-lingual image grounding using contrastive learning, improving object localization accuracy in low-resource languages.
                </p>
                <p>
                  <strong>Data Scientist</strong> — PrivateBlok (Feb 2023 – Nov 2023)<br>
                  I built a GPT-3.5-powered financial QA chatbot capable of answering queries about 10K+ private companies. By optimizing retrieval pipelines with a custom re-ranking algorithm, I reduced hallucinations and improved contextual accuracy. I also constructed a temporal knowledge graph of company data, enhancing the chatbot's ability to provide detailed financial insights and competitor analyses.
                </p>
                <p>
                  <strong>Project Assistant</strong> — Video Analytics Lab, IISc Bangalore (Aug 2022 – Jan 2023)<br>
                  For my thesis, I tackled the problem of long-tailed image generation using StyleGANs. By analyzing the W latent space and introducing controlled noise perturbations, I improved representation diversity and mitigated mode collapse, achieving a 19% FID improvement over prior methods. Our work was published at CVPR 2023, setting a new benchmark for long-tailed datasets like ImageNet-LT.
                </p>
                <p>
                  <strong>Software R&D Intern</strong> — Samsung R&D Institute India (May 2022 – Jul 2022)<br>
                  I developed a scalable system to detect objects associated with specific actions in images, achieving 82% precision and 67% recall. My work optimized transformer architectures for large-scale action recognition, enabling faster and more accurate detection in varied image galleries.
                </p>
              </td>
            </tr>
          </tbody></table>

          <!-- Independent Projects Section -->
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Independent Projects</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr onmouseout="wakeup_word_stop()" onmouseover="wakeup_word_start()"  bgcolor="#ffffd0">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='wakeup_word_image'><img src='images/500x300.png' width="160"></div>
          <img src='images/500x300.png' width="160">
        </div>
        <script type="text/javascript">
          function wakeup_word_start() {
            document.getElementById('wakeup_word_image').style.opacity = "1";
          }

          function wakeup_word_stop() {
            document.getElementById('wakeup_word_image').style.opacity = "0";
          }
          wakeup_word_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://github.com/KartikSharma907/Wake-up-Word-Detection">
          <span class="papertitle">Wake-up Word Detection</span>
        </a>
        <br>
        <strong>Kartik Sharma</strong>
        <br>
        <em>Independent Project</em>, July 2021
        <br>
        <a href="https://github.com/KartikSharma907/Wake-up-Word-Detection">project page</a>
        /
        <a href="https://github.com/KartikSharma907/Wake-up-Word-Detection">code</a>
        <p>Constructed a speech dataset from synthesized data and implemented a trigger word detection model with over 90% accuracy. Trained a GRU(Gated Recurrent Units) to detect when someone has finished saying the word "activate".</p>
      </td>
    </tr>

    <tr onmouseout="car_detect_stop()" onmouseover="car_detect_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='car_detect_image'><img src='images/car_detect_yolo.png' width="160"></div>
          <img src='images/car_detect_yolo.png' width="160">
        </div>
        <script type="text/javascript">
          function car_detect_start() {
            document.getElementById('car_detect_image').style.opacity = "1";
          }

          function car_detect_stop() {
            document.getElementById('car_detect_image').style.opacity = "0";
          }
          car_detect_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://github.com/KartikSharma907/Car-Detection-with-YOLO">
          <span class="papertitle">Car Detection with YOLO: You Only Look Once</span>
        </a>
        <br>
        <strong>Kartik Sharma</strong>
        <br>
        <em>Independent Project</em>, June 2021
        <br>
        <a href="https://github.com/KartikSharma907/Car-Detection-with-YOLO">project page</a>
        /
        <a href="https://github.com/KartikSharma907/Car-Detection-with-YOLO">code</a>
        <p>Implemented real-time object detection on a car dataset using the YOLO model, which was further improved using a U-net architecture. The YOLO model was stacked with Non-max suppression layers using IOU grid analysis to obtain the most accurate boundary boxes.</p>
      </td>
    </tr>

    <tr onmouseout="stonkmaster_stop()" onmouseover="stonkmaster_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='stonkmaster_image'><img src='images/500x300.png' width="160"></div>
          <img src='images/500x300.png' width="160">
        </div>
        <script type="text/javascript">
          function stonkmaster_start() {
            document.getElementById('stonkmaster_image').style.opacity = "1";
          }

          function stonkmaster_stop() {
            document.getElementById('stonkmaster_image').style.opacity = "0";
          }
          stonkmaster_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://github.com/KartikSharma907/Stock-Price-Simulator">
          <span class="papertitle">Stock Price Simulator</span>
        </a>
        <br>
        <strong>Kartik Sharma</strong>
        <br>
        <em>NIT Durgapur</em>, September 2020
        <br>
        <a href="https://github.com/KartikSharma907/Stock-Price-Simulator">project page</a>
        /
        <a href="https://github.com/KartikSharma907/Stock-Price-Simulator">code</a>
        <p>Developed a Machine Learning Model to predict real-time stock prices accurately using KNN, SVR, CART. Collected daily stock price data using REST API and analyzed the interdependence of different stock prices using LSTM (Long Short-Term Memory) networks.</p>
      </td>
    </tr>

    <tr onmouseout="nst_stop()" onmouseover="nst_start()">
      <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="two" id='nst_image'><img src='images/nst.png' width="160"></div>
          <img src='images/nst.png' width="160">
        </div>
        <script type="text/javascript">
          function nst_start() {
            document.getElementById('nst_image').style.opacity = "1";
          }

          function nst_stop() {
            document.getElementById('nst_image').style.opacity = "0";
          }
          nst_stop()
        </script>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://github.com/KartikSharma907/Neural-Style-Transfer">
          <span class="papertitle">Art Generation with Neural Style Transfer</span>
        </a>
        <br>
        <strong>Kartik Sharma</strong>
        <br>
        <em>Independent Project</em>, June 2021
        <br>
        <a href="https://github.com/KartikSharma907/Neural-Style-Transfer">project page</a>
        /
        <a href="https://github.com/KartikSharma907/Neural-Style-Transfer">code</a>
        <p>Used transfer learning on the VGG-19 network to generate new artistic images. Implemented a cost function that minimizes the content and style cost by running both the images through the pre-trained VGG-19 model.</p>
      </td>
    </tr>

          </tbody></table>

          
					<table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Experience</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #fcb97d;">
								 <h2>Research</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <strong>Research Assistant</strong> - Advanced Data Analytics and Parallel Technologies Lab, BITS Pilani<br>
                <em>May 2021 - Present</em><br>
                Estimating crop yield using multivariate time series analysis of fusion of Landsat and Sentinel satellite data.
                <br><br>
                <strong>Research Assistant</strong> - Central Electronics Engineering Research Institute (CSIR-CEERI), Pilani<br>
                <em>March 2021 - Present</em><br>
                Working on an instance segmentation framework to recognize highly cluttered instances of contraband items.
              </td>
            </tr>


            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #aaba9e;">
								 <h2>Industry</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <strong>AI Research Intern</strong> - Samsung Research Institute, Bangalore<br>
                <em>May 2021 - Present</em><br>
                Working on AI Video Understanding, making transformer-based multi-label classification models scalable and robust by incorporating dynamic querying.
              </td>
            </tr>

            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #c6b89e;">
								 <h2>Education</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <strong>B.E. Computer Science Engineering</strong> - BITS Pilani<br>
                <em>2018 - 2022</em><br>
                CGPA: 8.45/10.0
              </td>
            </tr>
						
						
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #edd892;">
								 <h2>Skills</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <strong>Programming Languages:</strong> Python, C++, Java, JavaScript<br>
                <strong>Machine Learning:</strong> TensorFlow, PyTorch, Scikit-learn, OpenCV<br>
                <strong>Deep Learning:</strong> CNN, RNN, LSTM, GRU, Transformers<br>
                <strong>Tools & Technologies:</strong> Git, Docker, AWS, Linux, MATLAB
              </td>
            </tr>
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Thank you for visiting my site, and if you have any inputs, I would love to hear from you!
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
